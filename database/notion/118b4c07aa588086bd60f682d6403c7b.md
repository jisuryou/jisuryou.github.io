# UX Evaluation of Text-Based Conversational Experiences (with LG)

Role: Project Manager
Skills: Project Management, Prompt Engineering, Python
Period: May 1, 2024 → August 31, 2024

---

![챗씽큐 프로젝트 이미지](/images/cxq.png)

### 프로젝트 요약

텍스트 기반 대화 경험의 UX 평가 — 참여 기간: 4개월(2024년 5월 ~ 2024년 8월)

목적: 홈IoT 커스텀 LLM 서비스 ‘챗씽큐(ChatThinQ)’의 대화 경험 품질을 평가하기 위한 자동화 평가 시스템 개발

프로젝트 과정: 평가 기준 수립 → 레퍼런스 발화 생성 → 평가 머신 개발 → 결과 분석 으로 진행

기여도: PM으로서 프로젝트 전 과정 리드

---

### 배경

문제 정의:

- LGE의 AI 이니셔티브의 첫 단계로 IoT 허브 앱 ThinQ에 대화형 인터페이스(chat)가 배치되어 생성형 AI 챗봇 '챗씽큐(ChatThinQ)'가 출시될 예정
- 대화형 인터페이스는 화면이든 보이스든 편리한 조작을 넘어 ‘지능’을 의미하므로, 단순히 기능이 성공하는 것보다 ‘대화 경험’이 중요
- 생성형 AI의 방대한 결과물을 기존의 사람 중심 평가 방식으로는 처리하기 어려워, LLM 활용 필요

목표 설정:

- 챗씽큐의 UX 평가 기준을 수립하고, 이를 바탕으로 자동화 평가 머신을 개발

---

### 주요 역할과 기여

(1) 프로젝트 관리 및 연구 방향 정립

- 목적: 챗씽큐 평가 시스템의 전반적인 연구 방향을 수립하고, 실행 전략을 기획
- 기여:
    - 홈IoT 챗봇의 LLM 기반 기능과 기존 IoT 기능을 명확히 구분하고, 평가 가능성과 기술적 한계를 분석하여 실현 가능한 평가 범위를 정의
    - 약 4,000개의 실제 사용자 발화 데이터를 기반으로 발화 종류와 맥락에 따른 기대치를 도출하고, UX 평가 기준의 우선순위를 체계화하여 평가팀과 개발/기획팀 간 협업을 조율
    - 평가 기준과 범위를 반영해 대화 시나리오에서 발생하는 맥락별 실패 원인을 분석하고, 이를 기반으로 평가 시스템의 설계 방향을 제안

(2) LLM 프롬프팅

- 목적: 다양한 사용 상황과 발화 패턴을 반영한 레퍼런스 발화 세트 구축 및 휴먼 평가를 모방하는 평가 프롬프트 작성
- 기여:
    - LLM 프롬프팅 기법을 활용해 실험실 데이터와 기획 문서를 기반으로 발화 데이터를 증강
        - 다양한 사용자 맥락과 특성을 반영하여 실제 상황을 모사한 레퍼런스 발화 세트를 생성
    - 평가 프롬프트를 설계하고, 실제 평가자와 비교 분석하며 프롬프트 성능을 지속적으로 개선

(3) 자동화 평가 시스템 개발

- 목적: 수립된 평가 기준과 데이터 세트를 활용하여 평가 프로세스를 자동화
- 기여:
    - python을 활용, 레퍼런스 발화 생성 프롬프트, 챗씽큐 응답, 평가 프롬프트를 통합하여 자동화 평가 머신 설계
    - 평가 머신의 실제 사용자인 개발/기획팀의 경험을 고려하여 평가 머신 인터페이스 설계

---

### 프로젝트 성과

성과:

- 자동화 평가 시스템으로 1,600여 개의 레퍼런스 세트에 대한 챗씽큐 응답 평가
- 평가 결과를 기반으로 챗씽큐 서비스 품질 개선 및 2차 산학 과제로 연계

인사이트:

- 커스텀 LLM 서비스의 성패는 제네릭 LLM을 서비스 목적에 맞게 얼마나 잘 커스터마이즈했는지에 달려있음. 즉, 서비스 영역의 특화된 질문에 대해 전문성 있는 답변이나 수행이 이루어져야 함을 확인
- LLM 프롬프팅을 통해 LLM의 가능성과 한계를 실질적으로 이해
# UX Evaluation of Text-Based Conversational Experiences (with LG)

Role: Project Manager
Skills: Project Management, Prompt Engineering, Python
Duration: 4 months (May 2024 - August 2024)

---

![Decision tree of evaluation prompts](/images/cxq.png)
*Decision tree of evaluation prompts*

### Project Summary

**Project Name:** UX Evaluation of Text-Based Conversational Experiences

**Project Purpose:** Development of an automated evaluation system to assess the conversational experience quality of home IoT custom LLM service 'ChatThinQ'

**Project Process:** Establishing evaluation criteria → Generating reference utterances → Developing evaluation machine → Analyzing results

**Contribution:** Led entire project process as PM

---

### Background

**Problem Definition:**

- As the first step in LGE's AI initiative, a conversational interface (chat) will be deployed to the IoT hub app ThinQ, launching the generative AI chatbot 'ChatThinQ'
- Whether screen-based or voice-based, conversational interfaces represent 'intelligence' beyond mere convenient operation, making the 'conversational experience' more important than mere functional success
- The vast output of generative AI is difficult to process with traditional human-centered evaluation methods, necessitating the use of LLM

**Goal Setting:**

- Establish UX evaluation criteria for ChatThinQ and develop an automated evaluation machine based on these criteria

---

### Key Roles and Contributions

**(1) Project Management**

**Purpose:** Establish overall research direction for ChatThinQ evaluation system and plan execution strategy

**Contributions:**

- Clearly distinguished between LLM-based functions and existing IoT functions of home IoT chatbot, analyzed evaluation feasibility and technical limitations to define achievable evaluation scope
- Based on actual user utterance data, derived expectations according to utterance types and contexts, systematized UX evaluation criteria priorities to coordinate collaboration between evaluation team and development/planning teams
- Analyzed contextual failure causes in conversation scenarios reflecting evaluation criteria and scope, and proposed core metrics and improvement directions for the evaluation system based on this

**(2) LLM Prompt Engineering**

**Purpose:** Build reference utterance sets reflecting various usage situations and utterance patterns, and write evaluation prompts that mimic human evaluation

**Contributions:**

- Augmented utterance data based on laboratory data and planning documents using LLM prompt engineering techniques
    - Generated reference utterance sets simulating real situations by reflecting various user contexts and characteristics
- Designed evaluation prompts and continuously improved prompt performance through comparative analysis with actual evaluators

**(3) Automated Evaluation System Development**

**Purpose:** Automate evaluation process using established evaluation criteria and data sets

**Contributions:**

- Designed automated evaluation machine integrating reference utterance generation prompts, ChatThinQ responses, and evaluation prompts using Python
- Designed evaluation machine interface considering the experience of actual users - development/planning teams

---

### Project Outcomes

**Achievements:**

- Evaluated ChatThinQ responses for 1,600+ reference sets using automated evaluation system
- Improved ChatThinQ service quality based on evaluation results and linked to second industry-academic project

**Insights:**

- Success of custom LLM services depends on how well generic LLMs are customized for service purposes. Confirmed that specialized answers or performance must be delivered for service domain-specific questions
- Gained practical understanding of LLM's possibilities and limitations through LLM prompt engineering
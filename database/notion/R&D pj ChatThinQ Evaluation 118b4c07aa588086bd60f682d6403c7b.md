# R&D pj: ChatThinQ Evaluation

Tags: Conversational AI
Link: https://ux.snu.ac.kr/project/2024/10/25/cxq.html
Role: Project Manager
Date: May 1, 2024 → August 31, 2024

### Summary

---

**텍스트 기반 대화 경험의 UX 평가 — 참여 기간: 4개월(2024년 5월 ~ 2024년 8월)**

과제 성과: 2차 과제로 연결

### Contributions

---

- 프로젝트는 평가 기준 수립 → 레퍼런스 발화 생성 → 평가 머신 개발 → 결과 분석 으로 진행
- **PM으로서 프로젝트 전 과정 리드**

### Background

---

- 배경:
    - LGE의 AI 이니셔티브의 첫 단계로 IoT 허브 앱 ThinQ에 대화형 인터페이스(chat)가 배치되어 생성형 AI 챗봇 '챗씽큐(ChatThinQ)'가 출시될 예정
    - 대화형 인터페이스는 화면이든 보이스든 편리한 조작을 넘어 ‘지능’을 의미하므로, 단순히 기능이 성공하는 것보다 ‘대화 경험’이 중요
    - 또한 생성 AI가 만들어내는 방대한 양의 결과물을 평가하는 데 한계가 있어, 사람이 직접 평가하는 기존 방식 대신 LLM을 활용한 자동 평가 시스템을 개발하고자 함
- 문제:
    - 챗씽큐에 적합한 UX 평가 기준과 평가 프롬프트는 무엇인가?

### Solutions

---

- 평가 기준 수립
    - 챗씽큐와 같은 커스텀 LLM 서비스의 성패는 제네릭 LLM을 서비스 목적에 맞게 얼마나 잘 커스터마이즈했는지에 달려있음. 즉, 서비스 영역의 특화된 질문에 대해 전문성 있는 답변이나 수행이 이루어져야 함
    - 이를 위해 기획서를 분석하고 사용자 발화를 수집하여 서비스 영역을 분류
    - 또한 연구자 3인이 사용자 발화에 대한 챗씽큐의 응답을 5점 척도로 평가하며 평가 기준을 수립
- 레퍼런스 발화 생성
    - 평가를 위한 사용자의 레퍼런스 발화 세트는 실제 상황의 다양성과 특수성, 그리고 빈번한 발화 패턴을 반영하여 구축해야 함
    - 현장 사용자의 실제 발화를 수집하는 것이 가장 이상적이지만, 시간적 제약으로 어려운 경우가 많음. 기획 문서를 바탕으로 LLM을 통해 발화를 생성하는 방법도 있으나, 이는 기획 의도에 한정된 발화만 생성되어 예상 시나리오 외의 상황을 평가하기 어려움
    - 본 과제에서는 실험실에서 수집한 발화를 기반으로 LLM 프롬프팅을 통해 데이터를 증강하여 레퍼런스 발화 세트를 구축
- 평가 머신 개발
    - 사람이 수립한 평가 기준을 바탕으로 평가 프롬프트를 작성
    - 실제 평가자의 결과와 지속적으로 비교하며 프롬프트를 개선
    - 레퍼런스 발화 생성 프롬프트, 챗씽큐 응답, 평가 프롬프트를 통합하여 평가 자동화 머신을 개발

### Insights

---

- 데이터 분석
    - 홈IoT 커스텀 LLM 챗봇의 서비스 영역은 다음과 같은 네 가지 축으로 분류되며, 각 영역별로 평가 기준이 상이함: 서비스 유형(홈IoT/LLM), 사용 단계(초기 기능 탐색/일상 활용/문제 해결), 상호작용 유형(기능/경험), 대화 여부(대화형/비대화형)
    - 홈IoT 커스텀 LLM 챗봇의 사용자 발화는 다음 다섯 가지 특징을 조합하여 생성될 수 있음: 도메인(가전제품 관련 질문 또는 칫챗 등의 챗봇 기능), 사용 단계(초기/일상/문제 상황), 맥락 정보(공간·날씨·시간 언급 여부), 기기 정보(LG 가전 제품군, 단일/복수 기기), 사용자 특성(숙련도와 가구원 구성)
- 인사이트
    -